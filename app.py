import os
import json
import logging
from flask import Flask, request, jsonify, render_template
from dotenv import load_dotenv
from datasets import load_dataset 
from tqdm.auto import tqdm 
import traceback 
import requests # Yeni eklenen: DoÄŸrudan API Ã§aÄŸrÄ±larÄ± iÃ§in
import numpy as np # Yeni eklenen: Embedding sonuÃ§larÄ± iÃ§in
from pinecone import Pinecone, ServerlessSpec 
from sentence_transformers import SentenceTransformer # Yeni eklenen: Bellek dostu embedding iÃ§in

# --- 1. LOGGING VE CONFIG ---
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

load_dotenv()

class Config:
    PINECONE_API_KEY = os.getenv("PINECONE_API_KEY")
    GEMINI_API_KEY = os.getenv("GOOGLE_API_KEY") or os.getenv("GEMINI_API_KEY")
    PINECONE_INDEX_NAME = os.getenv("PINECONE_INDEX", "psychotherapy-rag")
    PINECONE_ENV = os.getenv("PINECONE_ENV", "us-east-1") # Pinecone region'Ä±nÄ±zÄ± kontrol edin
    EMBEDDING_MODEL = "sentence-transformers/all-MiniLM-L6-v2" # Bellek aÅŸÄ±mÄ± durumunda bunu deÄŸiÅŸtirebilirsiniz
    EMBEDDING_DIM = 384
    MAX_RESPONSE_TOKENS = 4096 
    BATCH_SIZE = 100
    K_RETRIEVAL = 3

# --- 2. EMBEDDING SERVÄ°SÄ° (Bellek YÃ¼klemesini YÃ¶netmek Ä°Ã§in) ---
class EmbeddingService:
    def __init__(self, model_name: str):
        self.model = None
        logging.info(f"ğŸ”„ **2. Embedding Modeli:** '{model_name}' yÃ¼kleniyor...")
        try:
            # Model, STransformer ile yÃ¼klenerek bellek yÃ¼kÃ¼ optimize edilir.
            self.model = SentenceTransformer(model_name)
            logging.info("âœ… **2. Embedding Modeli TamamlandÄ±.**")
        except Exception as e:
            logging.error(f"âŒ HATA (Embedding): Model yÃ¼klenirken hata oluÅŸtu. {e}")
            raise RuntimeError("Embedding modeli yÃ¼klenemedi.")

    def embed(self, text: str):
        if not self.model:
            raise RuntimeError("Embedding modeli yÃ¼klenmedi.")
        # NumPy dizisini doÄŸrudan Pinecone iÃ§in List'e Ã§eviriyoruz
        return self.model.encode(text).tolist()

# --- 3. GEMINI MÃœÅTERÄ°SÄ° (DoÄŸrudan API EriÅŸimi Ä°Ã§in) ---
class GeminiClient:
    BASE_URL = "https://generativelanguage.googleapis.com/v1beta/models"

    def __init__(self, api_key: str, model_name: str = "gemini-2.5-flash"):
        self.api_key = api_key
        self.model = model_name
        self.headers = {"Content-Type": "application/json"}
        if not api_key:
            logging.warning("âš ï¸ Gemini API Key eksik. Mock yanÄ±tlar kullanÄ±lacak.")

    def generate(self, question: str, context: str):
        if not self.api_key:
            return "Mock Response: Gemini API Key not configured. Context: " + context

        url = f"{self.BASE_URL}/{self.model}:generateContent?key={self.api_key}"
        
        # BDT odaklÄ± sistem talimatÄ± (Sizin prompt'unuza dayalÄ±)
        system_instruction = f"""
            Sen, BiliÅŸsel DavranÄ±ÅŸÃ§Ä± Terapi (BDT) ilkelerine odaklanmÄ±ÅŸ, empatik ve etik kurallara baÄŸlÄ± bir Yapay Zeka Duygusal Rehbersin. 
            KullanÄ±cÄ±nÄ±n sorusuna yalnÄ±zca aÅŸaÄŸÄ±daki VERÄ° BAÄLAMI'nÄ± kullanarak BDT prensiplerine uygun, destekleyici ve rehberlik edici bir yanÄ±t ver.
            EÄŸer baÄŸlam yetersizse, etik kurallara baÄŸlÄ± kalarak genel bir BDT rehberliÄŸi yap.
            
            VERÄ° BAÄLLAMI:
            {context} 
            ---
            KullanÄ±cÄ±nÄ±n Sorusu:
        """
        
        # Payload oluÅŸtur
        payload = {
            "contents": [{"role": "user", "parts": [{"text": system_instruction + question}]}],
            "config": {
                "systemInstruction": system_instruction,
                "maxOutputTokens": Config.MAX_RESPONSE_TOKENS
            }
        }

        try:
            r = requests.post(url, headers=self.headers, json=payload, timeout=60)
            r.raise_for_status()
            data = r.json()
            
            # API yanÄ±tÄ±nÄ± gÃ¼venli ÅŸekilde Ã§Ã¶z
            if "candidates" in data and data["candidates"]:
                content = data["candidates"][0].get("content", {})
                if "parts" in content and content["parts"]:
                    return content["parts"][0].get("text", "Gemini'den yanÄ±t alÄ±namadÄ±.")
            return "Gemini'den geÃ§erli bir yanÄ±t gelmedi."

        except requests.exceptions.HTTPError as http_err:
            logging.error(f"âŒ Gemini HTTP hatasÄ±: {http_err} - {r.text}")
            return f"API HatasÄ±: HTTP {r.status_code}."
        except Exception as e:
            logging.error(f"âŒ Gemini API genel hatasÄ±: {e}")
            return "Gemini API baÄŸlantÄ± hatasÄ±."

# --- 4. PSÄ°KOTERAPÄ° ASÄ°STANI (Ana OrkestratÃ¶r) ---
class PsychotherapyAssistant:
    def __init__(self, cfg: Config):
        self.cfg = cfg
        # Bellek aÅŸÄ±mÄ± riskini azaltmak iÃ§in Embedding Modelini burada yÃ¼klÃ¼yoruz.
        self.embedder = EmbeddingService(cfg.EMBEDDING_MODEL) 
        self.gemini = GeminiClient(cfg.GEMINI_API_KEY)
        self.pinecone_index = self._setup_pinecone()
        self.documents = self._load_psychotherapy_data()
        self._load_dataset_to_pinecone() # Ä°ndeks boÅŸsa yÃ¼kleme yapar

    # Sizin load_psychotherapy_data fonksiyonunuz (DeÄŸiÅŸmedi)
    def _load_psychotherapy_data(self):
        DATASET_NAME = "Psychotherapy-LLM/CBT-Bench"
        SUBSET_NAME = "core_fine_test" 
        logging.info(f"ğŸ”„ **1. Veri YÃ¼kleme:** Hugging Face '{DATASET_NAME}' yÃ¼kleniyor...")
        # HafÄ±za problemini azaltmak iÃ§in "streaming=True" denenebilir, 
        # ancak burada tÃ¼m datayÄ± RAM'e alÄ±yoruz.
        try:
            dataset = load_dataset(DATASET_NAME, SUBSET_NAME, split="train") 
        except Exception as e:
            logging.error(f"âŒ HATA (Veri YÃ¼kleme): Hugging Face yÃ¼klenemedi. Hata: {e}")
            return []

        documents = []
        for i, row in enumerate(dataset):
             # ... (Veri temizleme mantÄ±ÄŸÄ± aynÄ±) ...
            situation = row.get('situation')
            thoughts = row.get('thoughts')
            core_belief = row.get('core_belief_fine_grained') 
            is_valid = situation and str(situation).strip() not in ['N/A', ''] and \
                       thoughts and str(thoughts).strip() not in ['N/A', '']
            
            if is_valid:
                content = (
                    f"Durum: {situation}. "
                    f"DanÄ±ÅŸan DÃ¼ÅŸÃ¼ncesi: {thoughts}. "
                    f"Ã‡ekirdek Ä°nanÃ§lar: {core_belief}"
                )
                documents.append({"id": str(i), "content": content})
        
        logging.info(f"âœ… **1. Veri YÃ¼kleme TamamlandÄ±:** Toplam {len(documents)} dokÃ¼man yÃ¼klendi.")
        return documents

    # Referans projedeki setup mantÄ±ÄŸÄ± (Pinecone Client)
    def _setup_pinecone(self):
        if not self.cfg.PINECONE_API_KEY: 
            logging.warning("âš ï¸ PINECONE_API_KEY eksik. Retrieval devre dÄ±ÅŸÄ±.")
            return None
        try:
            logging.info(f"ğŸ”„ **3. Pinecone BaÄŸlantÄ±sÄ±:** Pinecone istemcisi baÅŸlatÄ±lÄ±yor...")
            pc = Pinecone(api_key=self.cfg.PINECONE_API_KEY)
            index_name = self.cfg.PINECONE_INDEX_NAME
            
            if index_name not in pc.list_indexes().names:
                logging.warning(f"âš ï¸ **4a. Ä°ndeks OluÅŸturma:** '{index_name}' bulunamadÄ±. Yeni indeks oluÅŸturuluyor...")
                pc.create_index(
                    name=index_name,
                    dimension=self.cfg.EMBEDDING_DIM,
                    metric='cosine',
                    spec=ServerlessSpec(cloud='aws', region=self.cfg.PINECONE_ENV)
                )
            
            logging.info("âœ… **3. Pinecone BaÄŸlantÄ±sÄ± BaÅŸarÄ±lÄ±.**")
            return pc.Index(index_name)
        except Exception as e:
            logging.error(f"âŒ HATA (Pinecone): BaÅŸlatÄ±lamadÄ±. {e}")
            return None

    # Referans projedeki yÃ¼kleme mantÄ±ÄŸÄ±
    def _load_dataset_to_pinecone(self):
        if not self.pinecone_index: return
        
        vector_count = self.pinecone_index.describe_index_stats().get("total_vector_count", 0)
        if vector_count > 0:
            logging.info(f"âœ… **4a. Ä°ndeks KontrolÃ¼ BaÅŸarÄ±lÄ±:** {vector_count} vektÃ¶re sahip. YÃ¼kleme ATLANDI.")
            return

        logging.info(f"ğŸ”„ **4b. Veriler YÃ¼kleniyor:** {len(self.documents)} vektÃ¶r Pinecone'a yÃ¼kleniyor...")
        
        vectors = []
        try:
            for i, doc in enumerate(tqdm(self.documents)):
                # Embedding'i oluÅŸtur ve listeye ekle
                vectors.append((
                    doc["id"], 
                    self.embedder.embed(doc["content"]), 
                    {"text": doc["content"]} # Metadata sadece text'i tutsun
                ))
                
                if len(vectors) >= self.cfg.BATCH_SIZE:
                    self.pinecone_index.upsert(vectors=vectors)
                    vectors = [] # Batch temizle
            
            if vectors: # KalanlarÄ± yÃ¼kle
                self.pinecone_index.upsert(vectors=vectors)

            logging.info(f"âœ… **4b. Ä°ndeksleme TamamlandÄ±.**")
        except Exception as e:
            logging.error(f"âŒ HATA (Veri YÃ¼kleme/Upsert): {e}")

    # RAG sorgu mantÄ±ÄŸÄ± (Referans projedeki gibi, manuel olarak)
    def get_answer(self, question: str):
        if not question:
            return "LÃ¼tfen bir soru girin."

        context = "Relevant context not found."
        
        try:
            if self.pinecone_index:
                # 1. Sorgu Embedding'i
                query_emb = self.embedder.embed(question)
                
                # 2. Pinecone Sorgusu
                res = self.pinecone_index.query(
                    vector=query_emb, 
                    top_k=self.cfg.K_RETRIEVAL, 
                    include_metadata=True
                )
                
                # 3. BaÄŸlam OluÅŸturma (Sadece skor > 0.6 olanlarÄ± alalÄ±m)
                relevant_texts = [
                    m['metadata'].get('text', '') 
                    for m in res['matches'] 
                    if m['score'] > 0.6
                ]
                
                if relevant_texts:
                    context = "\n---\n".join(relevant_texts)
                    logging.info("ğŸ” Pinecone'dan baÄŸlam bulundu.")
                else:
                    logging.info("ğŸ” Pinecone'da yÃ¼ksek skorlu baÄŸlam bulunamadÄ±.")
            
            # 4. LLM YanÄ±tÄ±
            return self.gemini.generate(question, context)

        except RuntimeError as e: # Embedding hatasÄ±
            logging.error(f"âŒ RAG Runtime HatasÄ±: {e}")
            return "Hata: Embedding modelini kullanamÄ±yorum. Sunucu loglarÄ±nÄ± kontrol edin."
        except Exception as e:
            logging.error(f"âŒ RAG Genel HatasÄ±: {e}")
            traceback.print_exc() 
            return "Sunucu HatasÄ±: Sorgu yÃ¼rÃ¼tÃ¼lÃ¼rken beklenmeyen bir hata oluÅŸtu."

# --- 5. FLASK APP ---
app = Flask(__name__)

# Global asistan objesi (Gunicorn'da tekil kalacaktÄ±r)
try:
    cfg = Config()
    assistant = PsychotherapyAssistant(cfg)
    logging.info("âœ… **5. RAG Zinciri Kurulumu TamamlandÄ±!** Bot kullanÄ±ma hazÄ±r.")
except Exception as startup_error:
    logging.error(f"\n!!!! KRÄ°TÄ°K BAÅLANGIÃ‡ HATASI !!!!\nMesaj: {startup_error}")
    # BaÅŸarÄ±sÄ±z olursa 'assistant' deÄŸiÅŸkenini None bÄ±rakabiliriz veya bir Mock obje atayabiliriz.
    assistant = None 

@app.route("/")
def index():
    return render_template("index.html")

@app.route("/ask", methods=["POST"])
def ask_question():
    if assistant is None:
        return jsonify({"answer": "Error: RAG Chain initialization failed. Please check server logs for setup errors."}), 500
        
    data = request.get_json()
    query = data.get("question")
    
    if not query:
        return jsonify({"answer": "Please provide a question."}), 400

    try:
        logging.info(f"ğŸ”„ **Sorgu Ä°ÅŸleniyor:** '{query[:50]}...' iÃ§in RAG baÅŸlatÄ±ldÄ±.")
        answer = assistant.get_answer(query)
        logging.info("âœ… **Sorgu TamamlandÄ±:** YanÄ±t baÅŸarÄ±yla oluÅŸturuldu.")
        
        return jsonify({"answer": answer})

    except Exception as e:
        logging.error(f"âŒ KRÄ°TÄ°K HATA (RAG YÃ¼rÃ¼tme): Sorgu sÄ±rasÄ±nda hata oluÅŸtu.")
        traceback.print_exc() 
        
        return jsonify({
            "answer": f"API veya Sunucu HatasÄ±: Ä°ÅŸlem sÄ±rasÄ±nda beklenmeyen bir hata oluÅŸtu ({type(e).__name__})."
        }), 500


if __name__ == "__main__":
    # Render'Ä±n verdiÄŸi PORT deÄŸiÅŸkenini al
    port = int(os.environ.get("PORT", 5000)) 
    app.run(host="0.0.0.0", port=port)