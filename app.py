import os
import logging
import random
import requests
import numpy as np
from flask import Flask, request, jsonify, render_template
from dotenv import load_dotenv
from datasets import load_dataset 
from tqdm.auto import tqdm 
import traceback 
from pinecone import Pinecone, ServerlessSpec 
from sentence_transformers import SentenceTransformer 

# --- 1. LOGGING VE CONFIG ---
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

load_dotenv()

class Config:
    PINECONE_API_KEY = os.getenv("PINECONE_API_KEY")
    # Gemini API anahtarÄ± iÃ§in her iki ortam deÄŸiÅŸkenini de kontrol et
    GEMINI_API_KEY = os.getenv("GOOGLE_API_KEY") or os.getenv("GEMINI_API_KEY")
    PINECONE_INDEX_NAME = os.getenv("PINECONE_INDEX", "psychotherapy-rag")
    # Pinecone bÃ¶lgenizi kontrol edin. Kariyer Botu'ndaki gibi 'us-east-1' varsayÄ±lÄ±r.
    PINECONE_ENV = os.getenv("PINECONE_ENV", "us-east-1") 
    
    # BELLEK OPTÄ°MÄ°ZASYONU: Daha kÃ¼Ã§Ã¼k bir model kullanÄ±yoruz.
    EMBEDDING_MODEL = "sentence-transformers/paraphrase-MiniLM-L3-v2" # Boyut 384'tÃ¼r.
    EMBEDDING_DIM = 384
    
    MAX_RESPONSE_TOKENS = 4096 
    BATCH_SIZE = 100
    K_RETRIEVAL = 3 # KaÃ§ dokÃ¼man getirileceÄŸi

# --- 2. EMBEDDING SERVÄ°SÄ° (Hafif Model YÃ¼klemesi) ---
class EmbeddingService:
    def __init__(self, model_name: str):
        self.model = None
        logging.info(f"ğŸ”„ **2. Embedding Modeli:** '{model_name}' yÃ¼kleniyor...")
        try:
            # Model, STransformer ile yÃ¼klenerek bellek yÃ¼kÃ¼ optimize edilir.
            self.model = SentenceTransformer(model_name)
            logging.info("âœ… **2. Embedding Modeli TamamlandÄ±.**")
        except Exception as e:
            logging.error(f"âŒ HATA (Embedding): Model yÃ¼klenirken hata oluÅŸtu. {e}")
            raise RuntimeError("Embedding modeli yÃ¼klenemedi. Bellek limitini kontrol edin.")

    def embed(self, text: str):
        if not self.model:
            raise RuntimeError("Embedding modeli yÃ¼klenmedi.")
        # NumPy dizisini doÄŸrudan Pinecone iÃ§in List'e Ã§eviriyoruz
        return self.model.encode(text).tolist()

# --- 3. GEMINI MÃœÅTERÄ°SÄ° (Kariyer Botu'ndan alÄ±nan sade Gemini API yapÄ±sÄ±) ---
class GeminiClient:
    BASE_URL = "https://generativelanguage.googleapis.com/v1beta/models"

    def __init__(self, api_key: str, model_name: str = "gemini-2.0-flash"):
        self.api_key = api_key
        self.model = model_name
        self.headers = {"Content-Type": "application/json"}

    def generate(self, question: str, context: str):
        if not self.api_key:
            return "Mock Response: Gemini API Key not configured. Context: " + context

        url = f"{self.BASE_URL}/{self.model}:generateContent?key={self.api_key}"
        
        # Sizin orijinal BDT odaklÄ± sistem talimatÄ±nÄ±zÄ±n sadeleÅŸtirilmiÅŸ hali
        system_instruction = f"""
            Sen, BiliÅŸsel DavranÄ±ÅŸÃ§Ä± Terapi (BDT) ilkelerine odaklanmÄ±ÅŸ, empatik ve etik kurallara baÄŸlÄ± bir Yapay Zeka Duygusal Rehbersin. 
            KullanÄ±cÄ±nÄ±n sorusuna yalnÄ±zca aÅŸaÄŸÄ±daki VERÄ° BAÄLAMI'nÄ± kullanarak BDT prensiplerine uygun, destekleyici ve rehberlik edici bir yanÄ±t ver.
            EÄŸer baÄŸlam yetersizse, etik kurallara baÄŸlÄ± kalarak genel bir BDT rehberliÄŸi yap.
            
            VERÄ° BAÄLLAMI:
            {context} 
            ---
        """
        
        payload = {
            # Yeni Gemini API'da, sistem talimatÄ± 'contents' dÄ±ÅŸÄ±na konulur.
            "contents": [{"role": "user", "parts": [{"text": "KullanÄ±cÄ±nÄ±n Sorusu: " + question}]}],
            "config": {
                "systemInstruction": system_instruction,
                "maxOutputTokens": Config.MAX_RESPONSE_TOKENS
            }
        }

        try:
            r = requests.post(url, headers=self.headers, json=payload, timeout=60)
            r.raise_for_status()
            data = r.json()
            
            if "candidates" in data and data["candidates"]:
                content = data["candidates"][0].get("content", {})
                if "parts" in content and content["parts"]:
                    return content["parts"][0].get("text", "Gemini'den yanÄ±t alÄ±namadÄ±.")
            return "Gemini'den geÃ§erli bir yanÄ±t gelmedi."

        except Exception as e:
            logging.error(f"âŒ Gemini API hatasÄ±: {e}")
            return f"API HatasÄ±: Ä°ÅŸlem sÄ±rasÄ±nda hata oluÅŸtu ({type(e).__name__})."

# --- 4. PSÄ°KOTERAPÄ° ASÄ°STANI (Ana OrkestratÃ¶r) ---
class PsychotherapyAssistant:
    def __init__(self, cfg: Config):
        self.cfg = cfg
        # Kritik AdÄ±mlar: Bellek tÃ¼ketim sÄ±rasÄ±na gÃ¶re
        self.embedder = EmbeddingService(cfg.EMBEDDING_MODEL) 
        self.gemini = GeminiClient(cfg.GEMINI_API_KEY)
        self.documents = self._load_psychotherapy_data() # Veri yÃ¼klenir
        self.pinecone_index = self._setup_pinecone()
        self._load_dataset_to_pinecone() # Ä°ndeks boÅŸsa yÃ¼kleme yapar

    def _load_psychotherapy_data(self):
        # Orijinal Veri YÃ¼kleme Fonksiyonunuzun sadeleÅŸtirilmiÅŸ hali
        DATASET_NAME = "Psychotherapy-LLM/CBT-Bench"
        SUBSET_NAME = "core_fine_test" 
        logging.info(f"ğŸ”„ **1. Veri YÃ¼kleme:** Hugging Face '{DATASET_NAME}' yÃ¼kleniyor...")
        try:
            dataset = load_dataset(DATASET_NAME, SUBSET_NAME, split="train") 
        except Exception as e:
            logging.error(f"âŒ HATA (Veri YÃ¼kleme): Hugging Face yÃ¼klenemedi. Hata: {e}")
            return []

        documents = []
        for i, row in enumerate(dataset):
            situation = row.get('situation')
            thoughts = row.get('thoughts')
            core_belief = row.get('core_belief_fine_grained') 
            is_valid = situation and str(situation).strip() not in ['N/A', ''] and \
                       thoughts and str(thoughts).strip() not in ['N/A', '']
            
            if is_valid:
                content = (
                    f"Durum: {situation}. "
                    f"DanÄ±ÅŸan DÃ¼ÅŸÃ¼ncesi: {thoughts}. "
                    f"Ã‡ekirdek Ä°nanÃ§lar: {core_belief}"
                )
                documents.append({"id": str(i), "content": content})
        
        logging.info(f"âœ… **1. Veri YÃ¼kleme TamamlandÄ±:** Toplam {len(documents)} dokÃ¼man yÃ¼klendi.")
        return documents

    def _setup_pinecone(self):
        # Pinecone baÄŸlantÄ± ve indeks oluÅŸturma mantÄ±ÄŸÄ±
        if not self.cfg.PINECONE_API_KEY: 
            logging.warning("âš ï¸ PINECONE_API_KEY eksik. Retrieval devre dÄ±ÅŸÄ±.")
            return None
        try:
            logging.info(f"ğŸ”„ **3. Pinecone BaÄŸlantÄ±sÄ±:** Pinecone istemcisi baÅŸlatÄ±lÄ±yor...")
            pc = Pinecone(api_key=self.cfg.PINECONE_API_KEY)
            index_name = self.cfg.PINECONE_INDEX_NAME
            
            if index_name not in pc.list_indexes().names:
                logging.warning(f"âš ï¸ **4a. Ä°ndeks OluÅŸturma:** '{index_name}' bulunamadÄ±. Yeni indeks oluÅŸturuluyor...")
                pc.create_index(
                    name=index_name,
                    dimension=self.cfg.EMBEDDING_DIM,
                    metric='cosine',
                    spec=ServerlessSpec(cloud='aws', region=self.cfg.PINECONE_ENV)
                )
            
            logging.info("âœ… **3. Pinecone BaÄŸlantÄ±sÄ± BaÅŸarÄ±lÄ±.**")
            return pc.Index(index_name)
        except Exception as e:
            logging.error(f"âŒ HATA (Pinecone): BaÅŸlatÄ±lamadÄ±. {e}")
            return None

    def _load_dataset_to_pinecone(self):
        # KoÅŸullu yÃ¼kleme mantÄ±ÄŸÄ±
        if not self.pinecone_index: return
        
        vector_count = self.pinecone_index.describe_index_stats().get("total_vector_count", 0)
        if vector_count > 0:
            logging.info(f"âœ… **4a. Ä°ndeks KontrolÃ¼ BaÅŸarÄ±lÄ±:** {vector_count} vektÃ¶re sahip. YÃ¼kleme ATLANDI.")
            return

        logging.info(f"ğŸ”„ **4b. Veriler YÃ¼kleniyor:** {len(self.documents)} vektÃ¶r Pinecone'a yÃ¼kleniyor...")
        
        vectors = []
        try:
            for i, doc in enumerate(tqdm(self.documents)):
                vectors.append((
                    doc["id"], 
                    self.embedder.embed(doc["content"]), 
                    {"text": doc["content"]}
                ))
                
                if len(vectors) >= self.cfg.BATCH_SIZE:
                    self.pinecone_index.upsert(vectors=vectors)
                    vectors = []
            
            if vectors:
                self.pinecone_index.upsert(vectors=vectors)

            logging.info(f"âœ… **4b. Ä°ndeksleme TamamlandÄ±.**")
        except Exception as e:
            logging.error(f"âŒ HATA (Veri YÃ¼kleme/Upsert): {e}")

    def get_answer(self, question: str):
        # RAG (Retrieval) MantÄ±ÄŸÄ±
        if not question:
            return "LÃ¼tfen bir soru girin."

        context = "Relevant context not found."
        
        try:
            if self.pinecone_index:
                query_emb = self.embedder.embed(question)
                
                res = self.pinecone_index.query(
                    vector=query_emb, 
                    top_k=self.cfg.K_RETRIEVAL, 
                    include_metadata=True
                )
                
                # Sadece iyi eÅŸleÅŸen (0.6 Ã¼stÃ¼) dokÃ¼manlarÄ± al
                relevant_texts = [
                    m['metadata'].get('text', '') 
                    for m in res['matches'] 
                    if m['score'] > 0.6
                ]
                
                if relevant_texts:
                    context = "\n---\n".join(relevant_texts)
            
            # LLM (Generation) MantÄ±ÄŸÄ±
            return self.gemini.generate(question, context)

        except RuntimeError as e:
            logging.error(f"âŒ RAG Runtime HatasÄ±: {e}")
            return "Hata: Embedding modelini kullanamÄ±yorum. Sunucu loglarÄ±nÄ± kontrol edin."
        except Exception as e:
            logging.error(f"âŒ RAG Genel HatasÄ±: {e}")
            traceback.print_exc() 
            return "Sunucu HatasÄ±: Sorgu yÃ¼rÃ¼tÃ¼lÃ¼rken beklenmeyen bir hata oluÅŸtu."

# --- 5. FLASK APP ---
app = Flask(__name__)

# Global asistan objesi: Uygulama baÅŸlatÄ±lÄ±rken sadece bir kez kurulur.
try:
    cfg = Config()
    assistant = PsychotherapyAssistant(cfg)
    logging.info("==================================================")
    logging.info("âœ… **5. RAG Zinciri Kurulumu TamamlandÄ±!** Bot kullanÄ±ma hazÄ±r.")
    logging.info("==================================================")
except Exception as startup_error:
    logging.error(f"\n!!!! KRÄ°TÄ°K BAÅLANGIÃ‡ HATASI (502) !!!!")
    logging.error(f"Mesaj: {startup_error}")
    # Hata durumunda assistant'Ä± None bÄ±rakarak '/ask' endpoint'inde 500 hatasÄ± dÃ¶nÃ¼lmesini saÄŸlarÄ±z.
    assistant = None 

@app.route("/")
def index():
    return render_template("index.html")

@app.route("/ask", methods=["POST"])
def ask_question():
    if assistant is None:
        # BaÅŸlangÄ±Ã§ta baÅŸarÄ±sÄ±z olursa istemciye bilgi verilir.
        return jsonify({"answer": "Error: RAG Chain initialization failed. Please check server logs for setup errors."}), 500
        
    data = request.get_json()
    query = data.get("question")
    
    if not query:
        return jsonify({"answer": "Please provide a question."}), 400

    try:
        logging.info(f"ğŸ”„ **Sorgu Ä°ÅŸleniyor:** '{query[:50]}...' iÃ§in RAG baÅŸlatÄ±ldÄ±.")
        answer = assistant.get_answer(query)
        logging.info("âœ… **Sorgu TamamlandÄ±:** YanÄ±t baÅŸarÄ±yla oluÅŸturuldu.")
        
        return jsonify({"answer": answer})

    except Exception as e:
        logging.error(f"âŒ KRÄ°TÄ°K HATA (RAG YÃ¼rÃ¼tme): Sorgu sÄ±rasÄ±nda hata oluÅŸtu.")
        traceback.print_exc() 
        
        return jsonify({
            "answer": f"API veya Sunucu HatasÄ±: Ä°ÅŸlem sÄ±rasÄ±nda beklenmeyen bir hata oluÅŸtu ({type(e).__name__})."
        }), 500


if __name__ == "__main__":
    port = int(os.environ.get("PORT", 5000))
    app.run(host="0.0.0.0", port=port)