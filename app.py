# ---------------------------------------------------------------------------
# Proje: Kariyer Rehberi Sohbet Botu (Career Guide Chatbot)
#
# AmaÃ§:
# - Bu uygulama, RAG (Retrieval-Augmented Generation) mimarisiyle Ã§alÄ±ÅŸan
#   bir sohbet botu Ã¶rneÄŸidir. KullanÄ±cÄ±lardan gelen kariyerle ilgili
#   sorularÄ± yanÄ±tlamak iÃ§in halka aÃ§Ä±k bir Soru-Cevap veri kÃ¼mesini ve
#   Gemini dil modelini kullanÄ±r.
#
# Genel YapÄ±:
# - Uygulama, kullanÄ±cÄ±nÄ±n sorusuna en uygun yanÄ±tÄ± Ã¼retmek iÃ§in Ã¶nce
#   vektÃ¶r benzerliÄŸiyle ilgili verileri Pinecone Ã¼zerinden geri getirir
#   (retrieval), ardÄ±ndan bu verileri Gemini LLM ile birleÅŸtirerek
#   doÄŸal dilde yanÄ±t oluÅŸturur (generation).
#
# Temel BileÅŸenler:
# - EmbeddingService: Yerel olarak Ã§alÄ±ÅŸan "sentence-transformers/all-MiniLM-L6-v2"
#   modelini kullanarak metinleri sayÄ±sal vektÃ¶rlere dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r.
# - GeminiClient: Google Gemini APIâ€™sine istek gÃ¶nderen yardÄ±mcÄ± sÄ±nÄ±f.
# - Pinecone Entegrasyonu: VektÃ¶r benzerliÄŸi tabanlÄ± sorgulama iÃ§in kullanÄ±lÄ±r.
# - CareerAssistant: Geri getirme (retrieval) ve yanÄ±t Ã¼retme (generation)
#   iÅŸlemlerini yÃ¶neten ana bileÅŸendir.
#
# Notlar:
# - Uygulama CPU Ã¼zerinde Ã§alÄ±ÅŸacak ÅŸekilde optimize edilmiÅŸtir; GPU zorunlu deÄŸildir.
# - Ortam deÄŸiÅŸkenleri (.env dosyasÄ±nda) Ã¼zerinden API anahtarlarÄ± ayarlanmalÄ±dÄ±r.
# - GerÃ§ek bir LLM eriÅŸimi olmadan da demo amaÃ§lÄ± Ã§alÄ±ÅŸacak ÅŸekilde tasarlanmÄ±ÅŸtÄ±r.
#
# DaÄŸÄ±tÄ±m (Deploy) Bilgisi:
# - Kod Flask tabanlÄ± bir web uygulamasÄ± olarak yapÄ±landÄ±rÄ±lmÄ±ÅŸtÄ±r.
# - Render, Railway veya benzeri platformlarda doÄŸrudan Ã§alÄ±ÅŸtÄ±rÄ±labilir.
# - Gerekli baÄŸÄ±mlÄ±lÄ±klar requirements.txt dosyasÄ±nda listelenmiÅŸtir.
#
# DetaylÄ± kullanÄ±m, mimari aÃ§Ä±klama ve veri kÃ¼mesi bilgileri iÃ§in README.md dosyasÄ±na bakÄ±nÄ±z.
# ---------------------------------------------------------------------------


import os
from flask import Flask, request, jsonify, render_template
from dotenv import load_dotenv
import pandas as pd 
from datasets import load_dataset 
from tqdm.auto import tqdm 
import traceback 

# ==========================================================
# ğŸš¨ BELLEK VE DONANIM OPTÄ°MÄ°ZASYONU
# ==========================================================
# GPU varsa bile zorla CPU kullanÄ±mÄ±na yÃ¶nlendiriyoruz.
# Render gibi ortamlarda GPU bulunmadÄ±ÄŸÄ± iÃ§in bu gÃ¼venli bir Ã§Ã¶zÃ¼mdÃ¼r.
os.environ['CUDA_VISIBLE_DEVICES'] = '-1' 
os.environ['NO_GPUTILS'] = '1'

# ==========================================================
# ğŸ”— LangChain, Pinecone ve Gemini ModÃ¼l BaÄŸlantÄ±larÄ±
# ==========================================================
# LangChain: Zincir tabanlÄ± LLM entegrasyon frameworkâ€™Ã¼
# Pinecone: VektÃ¶r veritabanÄ±
# Gemini: Google Generative AI LLM modeli
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_pinecone import Pinecone as LangchainPinecone
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.schema import Document
from langchain_core.prompts import ChatPromptTemplate
from pinecone import Pinecone, ServerlessSpec 

# ==========================================================
# âš™ï¸ Flask UygulamasÄ± BaÅŸlatma
# ==========================================================
app = Flask(__name__)

# Ortam deÄŸiÅŸkenlerini (.env) yÃ¼kle
load_dotenv()

# ==========================================================
# ğŸ” Ortam DeÄŸiÅŸkenleri ve Genel Sabitler
# ==========================================================
PINECONE_API_KEY = os.getenv("PINECONE_API_KEY") 
GEMINI_API_KEY = os.getenv("GOOGLE_API_KEY") or os.getenv("GEMINI_API_KEY") 
PINECONE_INDEX_NAME = os.getenv("PINECONE_INDEX", "psychotherapy-rag") 
EMBEDDING_DIM = 384  # all-MiniLM-L6-v2 modeline uygun embedding boyutu
MAX_RESPONSE_TOKENS = 4096  # Geminiâ€™nin maksimum Ã§Ä±ktÄ± uzunluÄŸu

# Global deÄŸiÅŸkenler (zincir durumu burada tutulur)
qa_chain = None
retriever = None

# ==========================================================
# ğŸ§© 0ï¸âƒ£ Veri YÃ¼kleme Fonksiyonu (CBT-Bench Dataset)
# ==========================================================
def load_psychotherapy_data():
    """
    HuggingFace Ã¼zerinden BiliÅŸsel DavranÄ±ÅŸÃ§Ä± Terapi (CBT) veri kÃ¼mesini indirir.
    Eksik ya da boÅŸ verileri atar, her satÄ±rÄ± Document objesine Ã§evirir.
    """
    DATASET_NAME = "Psychotherapy-LLM/CBT-Bench"
    SUBSET_NAME = "core_fine_test" 
    print(f"ğŸ”„ **1. Veri YÃ¼kleme:** Hugging Face '{DATASET_NAME}' ({SUBSET_NAME}) alt kÃ¼mesi yÃ¼kleniyor...")

    try:
        dataset = load_dataset(DATASET_NAME, SUBSET_NAME, split="train") 
    except Exception as e:
        print(f"âŒ **HATA (Veri YÃ¼kleme):** Hugging Face veri seti yÃ¼klenemedi. Hata: {e}")
        return []

    documents = []
    discarded_record_count = 0
    
    for i, row in enumerate(dataset):
        situation = row.get('situation')
        thoughts = row.get('thoughts')
        core_belief = row.get('core_belief_fine_grained') 
        
        # GeÃ§erli kayÄ±tlarÄ± filtrele
        is_situation_valid = situation and str(situation).strip() not in ['N/A', '']
        is_thoughts_valid = thoughts and str(thoughts).strip() not in ['N/A', '']

        # GeÃ§erli verilerden Document objesi oluÅŸtur
        if is_situation_valid and is_thoughts_valid:
            content = (
                f"**Durum:** {situation}. "
                f"**DanÄ±ÅŸan DÃ¼ÅŸÃ¼ncesi:** {thoughts}. "
                f"**Ã‡ekirdek Ä°nanÃ§lar:** {core_belief}"
            )
            documents.append(Document(
                page_content=content, 
                metadata={"id": str(i), "situation_summary": situation}
            ))
        else:
            discarded_record_count += 1
    
    print(f"âœ… **1. Veri YÃ¼kleme TamamlandÄ±:** {len(documents)} anlamlÄ± dokÃ¼man yÃ¼klendi.")
    if discarded_record_count > 0:
        print(f"âš ï¸ {discarded_record_count} adet eksik bilgi iÃ§eren kayÄ±t atÄ±ldÄ±.")
    return documents


# ==========================================================
# ğŸ§  1ï¸âƒ£ RAG Zinciri BaÅŸlatma Fonksiyonu
# ==========================================================
def initialize_rag_chain():
    """
    RAG pipelineâ€™Ä±nÄ± kurar:
    - Veri kÃ¼mesini yÃ¼kler
    - all-MiniLM-L6-v2 modelinden embedding Ã¼retir
    - Pineconeâ€™a yÃ¼kler veya var olan indeksi baÄŸlar
    - Gemini LLMâ€™i retriever zinciriyle baÄŸlar
    """
    global qa_chain, retriever

    # Ortam deÄŸiÅŸkenlerinin doÄŸruluÄŸunu kontrol et
    if not PINECONE_API_KEY or not GEMINI_API_KEY:
        raise ValueError("API AnahtarlarÄ± eksik. LÃ¼tfen Render Ortam DeÄŸiÅŸkenlerini kontrol edin.")

    # 1ï¸âƒ£ Veri kÃ¼mesini yÃ¼kle
    documents = load_psychotherapy_data()
    if not documents:
        raise ValueError("Veri kÃ¼mesinden yÃ¼klenecek geÃ§erli dokÃ¼man bulunamadÄ±.")
    
    TOTAL_DOCUMENT_COUNT = len(documents)
    
    # 2ï¸âƒ£ Embedding Modeli (all-MiniLM-L6-v2)
    MODEL_NAME_OPTIMIZED = "sentence-transformers/all-MiniLM-L6-v2"
    print(f"ğŸ”„ **2. Embedding Modeli:** '{MODEL_NAME_OPTIMIZED}' yÃ¼kleniyor (CPU modunda)...")

    try:
        embeddings = HuggingFaceEmbeddings(
            model_name=MODEL_NAME_OPTIMIZED,
            model_kwargs={'device': 'cpu'}  # GPU yerine CPU kullan
        )
        print("âœ… **2. Embedding Modeli BaÅŸarÄ±yla YÃ¼klendi.**")
    except Exception as e:
        raise Exception(f"HATA (Embedding): Model yÃ¼klenirken hata oluÅŸtu. {e}")

    # 3ï¸âƒ£ Pinecone BaÄŸlantÄ±sÄ±
    try:
        print(f"ğŸ”„ **3. Pinecone BaÄŸlantÄ±sÄ±:** Pinecone istemcisi baÅŸlatÄ±lÄ±yor...")
        pc = Pinecone(api_key=PINECONE_API_KEY)
        print("âœ… **3. Pinecone BaÄŸlantÄ±sÄ± BaÅŸarÄ±lÄ±.**")
        
        index_name = PINECONE_INDEX_NAME
        index_exists = index_name in [i["name"] for i in pc.list_indexes()]
        should_upsert = True
        
        # EÄŸer indeks zaten varsa, tekrar yÃ¼klememek iÃ§in kontrol et
        if index_exists:
            current_index = pc.Index(index_name)
            vector_count = current_index.describe_index_stats().get('total_vector_count', 0)
            
            if vector_count > 0:
                print(f"âœ… '{index_name}' indeksi {vector_count} vektÃ¶re sahip. Yeniden yÃ¼kleme atlandÄ±.")
                should_upsert = False
            else:
                print(f"âš ï¸ Ä°ndeks mevcut ama boÅŸ. Yeniden yÃ¼kleme baÅŸlatÄ±lÄ±yor.")
        else:
            # Ä°ndeks yoksa yeni oluÅŸtur
            print(f"âš ï¸ '{index_name}' bulunamadÄ±. Yeni indeks oluÅŸturuluyor...")
            pc.create_index(
                name=index_name,
                dimension=EMBEDDING_DIM,
                metric='cosine',
                spec=ServerlessSpec(cloud='aws', region='us-east-1')
            )
            current_index = pc.Index(index_name) 
        
        # 4ï¸âƒ£ Verileri Pineconeâ€™a yÃ¼kle
        if should_upsert:
            print(f"ğŸ”„ **4. VektÃ¶r YÃ¼kleme:** {TOTAL_DOCUMENT_COUNT} belge Pineconeâ€™a aktarÄ±lÄ±yor...")
            batch_size = 100
            for i in tqdm(range(0, TOTAL_DOCUMENT_COUNT, batch_size)):
                batch = documents[i:min(i + batch_size, TOTAL_DOCUMENT_COUNT)]
                texts = [doc.page_content for doc in batch]
                vectors = embeddings.embed_documents(texts)
                to_upsert = [(str(doc.metadata['id']), vectors[j], doc.metadata) 
                             for j, doc in enumerate(batch)]
                current_index.upsert(vectors=to_upsert)
            print(f"âœ… **4. Ä°ndeksleme TamamlandÄ±.**")
        
        # Final kontrol
        final_index = pc.Index(index_name)
        final_vector_count = final_index.describe_index_stats().get('total_vector_count', 'Bilinmiyor')
        print(f"âœ¨ Pinecone Ä°ndeks Durumu: {final_vector_count} toplam vektÃ¶r mevcut.")
        
        # LangChain iÃ§in retriever oluÅŸtur
        vector_store = LangchainPinecone.from_existing_index(
            index_name=index_name,
            embedding=embeddings
        )
        retriever = vector_store.as_retriever(search_type="similarity", search_kwargs={"k": 3})

    except Exception as e:
        raise Exception(f"HATA (Pinecone Zinciri): Pinecone baÅŸlatÄ±lamadÄ±. {e}")
    
    # 5ï¸âƒ£ Gemini LLM ile Zincir Kurulumu
    print("ğŸ”„ **5. LLM BaÄŸlantÄ±sÄ±:** Gemini (gemini-2.0-flash) modeli baÅŸlatÄ±lÄ±yor...")
    try:
        llm = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash", 
            google_api_key=GEMINI_API_KEY,
            max_output_tokens=MAX_RESPONSE_TOKENS 
        )

        # Sistem promptu: TerapÃ¶tik ve empatik yanÄ±t Ã¼retimi
        SYSTEM_PROMPT_TEMPLATE = """
            Sen, BiliÅŸsel DavranÄ±ÅŸÃ§Ä± Terapi (BDT) ilkelerine odaklanmÄ±ÅŸ, empatik ve etik kurallara baÄŸlÄ± bir Yapay Zeka Duygusal Rehbersin. 
            AÅŸaÄŸÄ±daki VERÄ° BAÄLLAMI'nÄ± kullanarak kullanÄ±cÄ± sorusuna destekleyici ve rehberlik edici bir yanÄ±t ver.
            VERÄ° BAÄLLAMI:
            {context} 
            ---
        """

        prompt = ChatPromptTemplate.from_messages([
            ("system", SYSTEM_PROMPT_TEMPLATE),
            ("human", "{input}")
        ])

        # LLM + Retriever zinciri (RAG) oluÅŸturma
        document_chain = create_stuff_documents_chain(llm, prompt) 
        qa_chain = create_retrieval_chain(retriever=retriever, combine_docs_chain=document_chain)

        print("âœ… **5. RAG Zinciri BaÅŸarÄ±yla Kuruldu.** Bot kullanÄ±ma hazÄ±r.")

    except Exception as e:
        raise Exception(f"HATA (Gemini LLM): Gemini baÅŸlatÄ±lÄ±rken hata oluÅŸtu. {e}")


# ==========================================================
# ğŸŒ 2ï¸âƒ£ Flask UÃ§ NoktalarÄ± (Routes)
# ==========================================================

# Uygulama baÅŸlarken zinciri baÅŸlat
try:
    print("==================================================")
    print("ğŸš€ Flask RAG Psikoterapi Botu BaÅŸlatÄ±lÄ±yor...")
    initialize_rag_chain()
    print("==================================================")

except Exception as startup_error:
    print(f"\n\n\n!!!! KRÄ°TÄ°K BAÅLANGIÃ‡ HATASI !!!!\n\n")
    print(f"Hata Tipi: {type(startup_error).__name__}")
    print(f"Mesaj: {startup_error}")
    print("Render Ortam DeÄŸiÅŸkenlerinizi kontrol edin (PINECONE_API_KEY, GOOGLE_API_KEY).")
    print(f"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n\n")
    
# Ana sayfa (index.html)
@app.route("/")
def index():
    """Ana sayfayÄ± (HTML arayÃ¼zÃ¼) dÃ¶ndÃ¼rÃ¼r."""
    return render_template("index.html")

# Soru-cevap endpointâ€™i
@app.route("/ask", methods=["POST"])
def ask_question():
    """KullanÄ±cÄ± sorusunu alÄ±r, RAG zincirini Ã§alÄ±ÅŸtÄ±rÄ±r, yanÄ±t dÃ¶ndÃ¼rÃ¼r."""
    global qa_chain
    
    if qa_chain is None:
        return jsonify({"answer": "RAG Chain baÅŸlatÄ±lamadÄ±. Sunucu loglarÄ±nÄ± kontrol edin."}), 500
        
    data = request.get_json()
    query = data.get("question")
    
    if not query:
        return jsonify({"answer": "LÃ¼tfen bir soru gÃ¶nderin."}), 400

    try:
        print(f"ğŸ”„ **Sorgu Ä°ÅŸleniyor:** '{query[:50]}...'")
        response = qa_chain.invoke({"input": query})
        answer = response.get("answer")
        
        if not answer:
            print("âš ï¸ Gemini yanÄ±t Ã¼retmedi. Context kontrol ediliyor...")
            return jsonify({
                "answer": "Yapay zeka anlamlÄ± bir yanÄ±t oluÅŸturamadÄ±. LÃ¼tfen soruyu yeniden formÃ¼le edin."
            }), 500
        
        print("âœ… **YanÄ±t Ãœretildi:** BaÅŸarÄ±lÄ±.")
        return jsonify({"answer": answer})

    except Exception as e:
        print(f"âŒ **HATA:** Sorgu sÄ±rasÄ±nda hata oluÅŸtu.")
        traceback.print_exc() 
        return jsonify({
            "answer": f"Sunucu hatasÄ±: {type(e).__name__}"
        }), 500


# ==========================================================
# ğŸ UygulamayÄ± BaÅŸlat (Yerel test iÃ§in)
# ==========================================================
if __name__ == "__main__":
    port = int(os.environ.get("PORT", 5000)) 
    app.run(host="0.0.0.0", port=port)
