import os
import logging
import random
import requests
import numpy as np
from flask import Flask, request, jsonify, render_template
from dotenv import load_dotenv
from datasets import load_dataset 
from tqdm.auto import tqdm 
import traceback 
from pinecone import Pinecone, ServerlessSpec 
from sentence_transformers import SentenceTransformer 

# ğŸš¨ KRÄ°TÄ°K BELLEK OPTÄ°MÄ°ZASYONU: SADECE CPU'YU ZORLA
# Bu, PyTorch'un GPU bileÅŸenlerini yÃ¼klemesini engeller ve RAM kullanÄ±mÄ±nÄ± azaltÄ±r.
os.environ['TRANSFORMERS_NO_ADVICE'] = '1'
os.environ['CUDA_VISIBLE_DEVICES'] = '-1' 
os.environ['NO_GPUTILS'] = '1' 

# Gerekirse, PyTorch'un dÃ¼ÅŸÃ¼k bellekli modunu zorlamak iÃ§in bu eklenebilir
# import torch
# torch.set_num_threads(1) 

# --- 1. LOGGING VE CONFIG ---
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

load_dotenv()

class Config:
    PINECONE_API_KEY = os.getenv("PINECONE_API_KEY")
    GEMINI_API_KEY = os.getenv("GOOGLE_API_KEY") or os.getenv("GEMINI_API_KEY")
    PINECONE_INDEX_NAME = os.getenv("PINECONE_INDEX", "psychotherapy-rag")
    PINECONE_ENV = os.getenv("PINECONE_ENV", "us-east-1") 
    
    # EN HAFÄ°F MODELLERDEN BÄ°RÄ° (Boyut 384)
    EMBEDDING_MODEL = "sentence-transformers/paraphrase-MiniLM-L3-v2" 
    EMBEDDING_DIM = 384
    
    MAX_RESPONSE_TOKENS = 4096 
    BATCH_SIZE = 100
    K_RETRIEVAL = 3 

# --- 2. EMBEDDING SERVÄ°SÄ° (Hafif Model YÃ¼klemesi) ---
class EmbeddingService:
    def __init__(self, model_name: str):
        self.model = None
        logging.info(f"ğŸ”„ **2. Embedding Modeli:** '{model_name}' yÃ¼kleniyor...")
        try:
            # Bellek optimizasyonlu model yÃ¼klemesi
            self.model = SentenceTransformer(model_name)
            logging.info("âœ… **2. Embedding Modeli TamamlandÄ± (CPU).**")
        except Exception as e:
            logging.error(f"âŒ HATA (Embedding): Model yÃ¼klenirken hata oluÅŸtu. {e}")
            raise RuntimeError("Embedding modeli yÃ¼klenemedi. Bellek limitini kontrol edin.")

    def embed(self, text: str):
        if not self.model:
            raise RuntimeError("Embedding modeli yÃ¼klenmedi.")
        return self.model.encode(text).tolist()

# --- 3. GEMINI MÃœÅTERÄ°SÄ° (DeÄŸiÅŸmedi) ---
class GeminiClient:
    BASE_URL = "https://generativelanguage.googleapis.com/v1beta/models"

    def __init__(self, api_key: str, model_name: str = "gemini-2.5-flash"):
        self.api_key = api_key
        self.model = model_name
        self.headers = {"Content-Type": "application/json"}

    def generate(self, question: str, context: str):
        if not self.api_key:
            return "Mock Response: Gemini API Key not configured. Context: " + context

        url = f"{self.BASE_URL}/{self.model}:generateContent?key={self.api_key}"
        
        system_instruction = f"""
            Sen, BiliÅŸsel DavranÄ±ÅŸÃ§Ä± Terapi (BDT) ilkelerine odaklanmÄ±ÅŸ, empatik ve etik kurallara baÄŸlÄ± bir Yapay Zeka Duygusal Rehbersin. 
            KullanÄ±cÄ±nÄ±n sorusuna yalnÄ±zca aÅŸaÄŸÄ±daki VERÄ° BAÄLAMI'nÄ± kullanarak BDT prensiplerine uygun, destekleyici ve rehberlik edici bir yanÄ±t ver.
            EÄŸer baÄŸlam yetersizse, etik kurallara baÄŸlÄ± kalarak genel bir BDT rehberliÄŸi yap.
            
            VERÄ° BAÄLLAMI:
            {context} 
            ---
        """
        
        payload = {
            "contents": [{"role": "user", "parts": [{"text": "KullanÄ±cÄ±nÄ±n Sorusu: " + question}]}],
            "config": {
                "systemInstruction": system_instruction,
                "maxOutputTokens": Config.MAX_RESPONSE_TOKENS
            }
        }

        try:
            r = requests.post(url, headers=self.headers, json=payload, timeout=60)
            r.raise_for_status()
            data = r.json()
            
            if "candidates" in data and data["candidates"]:
                content = data["candidates"][0].get("content", {})
                if "parts" in content and content["parts"]:
                    return content["parts"][0].get("text", "Gemini'den yanÄ±t alÄ±namadÄ±.")
            return "Gemini'den geÃ§erli bir yanÄ±t gelmedi."

        except Exception as e:
            logging.error(f"âŒ Gemini API hatasÄ±: {e}")
            return f"API HatasÄ±: Ä°ÅŸlem sÄ±rasÄ±nda hata oluÅŸtu ({type(e).__name__})."

# --- 4. PSÄ°KOTERAPÄ° ASÄ°STANI (RAG MantÄ±ÄŸÄ± DeÄŸiÅŸmedi) ---
class PsychotherapyAssistant:
    def __init__(self, cfg: Config):
        self.cfg = cfg
        self.embedder = EmbeddingService(cfg.EMBEDDING_MODEL) 
        self.gemini = GeminiClient(cfg.GEMINI_API_KEY)
        self.documents = self._load_psychotherapy_data() 
        self.pinecone_index = self._setup_pinecone()
        self._load_dataset_to_pinecone() 

    def _load_psychotherapy_data(self):
        # Hugging Face veri seti yÃ¼kleme ve temizleme mantÄ±ÄŸÄ± (Bellek aÅŸÄ±mÄ±na raÄŸmen korunur)
        DATASET_NAME = "Psychotherapy-LLM/CBT-Bench"
        SUBSET_NAME = "core_fine_test" 
        logging.info(f"ğŸ”„ **1. Veri YÃ¼kleme:** Hugging Face '{DATASET_NAME}' yÃ¼kleniyor...")
        try:
            # Bu kÄ±sÄ±m hala potansiyel bir bellek tÃ¼keticisidir.
            dataset = load_dataset(DATASET_NAME, SUBSET_NAME, split="train") 
        except Exception as e:
            logging.error(f"âŒ HATA (Veri YÃ¼kleme): Hugging Face yÃ¼klenemedi. Hata: {e}")
            return []

        documents = []
        for i, row in enumerate(dataset):
            situation = row.get('situation')
            thoughts = row.get('thoughts')
            core_belief = row.get('core_belief_fine_grained') 
            is_valid = situation and str(situation).strip() not in ['N/A', ''] and \
                       thoughts and str(thoughts).strip() not in ['N/A', '']
            
            if is_valid:
                content = (
                    f"Durum: {situation}. "
                    f"DanÄ±ÅŸan DÃ¼ÅŸÃ¼ncesi: {thoughts}. "
                    f"Ã‡ekirdek Ä°nanÃ§lar: {core_belief}"
                )
                documents.append({"id": str(i), "content": content})
        
        logging.info(f"âœ… **1. Veri YÃ¼kleme TamamlandÄ±:** Toplam {len(documents)} dokÃ¼man yÃ¼klendi.")
        return documents

    def _setup_pinecone(self):
        if not self.cfg.PINECONE_API_KEY: 
            logging.warning("âš ï¸ PINECONE_API_KEY eksik. Retrieval devre dÄ±ÅŸÄ±.")
            return None
        try:
            logging.info(f"ğŸ”„ **3. Pinecone BaÄŸlantÄ±sÄ±:** Pinecone istemcisi baÅŸlatÄ±lÄ±yor...")
            pc = Pinecone(api_key=self.cfg.PINECONE_API_KEY)
            index_name = self.cfg.PINECONE_INDEX_NAME
            
            if index_name not in pc.list_indexes().names:
                logging.warning(f"âš ï¸ **4a. Ä°ndeks OluÅŸturma:** '{index_name}' bulunamadÄ±. Yeni indeks oluÅŸturuluyor...")
                pc.create_index(
                    name=index_name,
                    dimension=self.cfg.EMBEDDING_DIM,
                    metric='cosine',
                    spec=ServerlessSpec(cloud='aws', region=self.cfg.PINECONE_ENV)
                )
            
            logging.info("âœ… **3. Pinecone BaÄŸlantÄ±sÄ± BaÅŸarÄ±lÄ±.**")
            return pc.Index(index_name)
        except Exception as e:
            logging.error(f"âŒ HATA (Pinecone): BaÅŸlatÄ±lamadÄ±. {e}")
            return None

    def _load_dataset_to_pinecone(self):
        if not self.pinecone_index: return
        
        vector_count = self.pinecone_index.describe_index_stats().get("total_vector_count", 0)
        if vector_count > 0:
            logging.info(f"âœ… **4a. Ä°ndeks KontrolÃ¼ BaÅŸarÄ±lÄ±:** {vector_count} vektÃ¶re sahip. YÃ¼kleme ATLANDI.")
            return

        logging.info(f"ğŸ”„ **4b. Veriler YÃ¼kleniyor:** {len(self.documents)} vektÃ¶r Pinecone'a yÃ¼kleniyor...")
        
        vectors = []
        try:
            for i, doc in enumerate(tqdm(self.documents)):
                vectors.append((
                    doc["id"], 
                    self.embedder.embed(doc["content"]), 
                    {"text": doc["content"]}
                ))
                
                if len(vectors) >= self.cfg.BATCH_SIZE:
                    self.pinecone_index.upsert(vectors=vectors)
                    vectors = []
            
            if vectors:
                self.pinecone_index.upsert(vectors=vectors)

            logging.info(f"âœ… **4b. Ä°ndeksleme TamamlandÄ±.**")
        except Exception as e:
            logging.error(f"âŒ HATA (Veri YÃ¼kleme/Upsert): {e}")

    def get_answer(self, question: str):
        if not question:
            return "LÃ¼tfen bir soru girin."

        context = "Relevant context not found."
        
        try:
            if self.pinecone_index:
                query_emb = self.embedder.embed(question)
                
                res = self.pinecone_index.query(
                    vector=query_emb, 
                    top_k=self.cfg.K_RETRIEVAL, 
                    include_metadata=True
                )
                
                relevant_texts = [
                    m['metadata'].get('text', '') 
                    for m in res['matches'] 
                    if m['score'] > 0.6
                ]
                
                if relevant_texts:
                    context = "\n---\n".join(relevant_texts)
            
            return self.gemini.generate(question, context)

        except RuntimeError as e:
            logging.error(f"âŒ RAG Runtime HatasÄ±: {e}")
            return "Hata: Embedding modelini kullanamÄ±yorum. Sunucu loglarÄ±nÄ± kontrol edin."
        except Exception as e:
            logging.error(f"âŒ RAG Genel HatasÄ±: {e}")
            traceback.print_exc() 
            return "Sunucu HatasÄ±: Sorgu yÃ¼rÃ¼tÃ¼lÃ¼rken beklenmeyen bir hata oluÅŸtu."

# --- 5. FLASK APP ---
app = Flask(__name__)

try:
    cfg = Config()
    logging.info("==================================================")
    logging.info("ğŸš€ Flask RAG Psikoterapi Botu BaÅŸlatÄ±lÄ±yor...")
    assistant = PsychotherapyAssistant(cfg)
    logging.info("âœ… **5. RAG Zinciri Kurulumu TamamlandÄ±!** Bot kullanÄ±ma hazÄ±r.")
    logging.info("==================================================")
except Exception as startup_error:
    # Bu hata, Gunicorn'a fÄ±rlatÄ±lacak ve 502/503 hatasÄ± verecektir.
    logging.error(f"\n!!!! KRÄ°TÄ°K BAÅLANGIÃ‡ HATASI (502) !!!!")
    logging.error(f"Mesaj: {startup_error}")
    # traceback.print_exc() # DetaylÄ± hata iÃ§in
    assistant = None 

@app.route("/")
def index():
    return render_template("index.html")

@app.route("/ask", methods=["POST"])
def ask_question():
    if assistant is None:
        return jsonify({"answer": "Error: RAG Chain initialization failed. Please check server logs for setup errors."}), 500
        
    data = request.get_json()
    query = data.get("question")
    
    if not query:
        return jsonify({"answer": "Please provide a question."}), 400

    try:
        logging.info(f"ğŸ”„ **Sorgu Ä°ÅŸleniyor:** '{query[:50]}...' iÃ§in RAG baÅŸlatÄ±ldÄ±.")
        answer = assistant.get_answer(query)
        logging.info("âœ… **Sorgu TamamlandÄ±:** YanÄ±t baÅŸarÄ±yla oluÅŸturuldu.")
        
        return jsonify({"answer": answer})

    except Exception as e:
        logging.error(f"âŒ KRÄ°TÄ°K HATA (RAG YÃ¼rÃ¼tme): Sorgu sÄ±rasÄ±nda hata oluÅŸtu.")
        traceback.print_exc() 
        
        return jsonify({
            "answer": f"API veya Sunucu HatasÄ±: Ä°ÅŸlem sÄ±rasÄ±nda beklenmeyen bir hata oluÅŸtu ({type(e).__name__})."
        }), 500


if __name__ == "__main__":
    port = int(os.environ.get("PORT", 5000))
    app.run(host="0.0.0.0", port=port)