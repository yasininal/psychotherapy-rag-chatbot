import os
from flask import Flask, request, jsonify, render_template
from dotenv import load_dotenv
import pandas as pd 
from datasets import load_dataset 
from tqdm.auto import tqdm 
import traceback 

# LangChain, Pinecone ve Gemini ModÃ¼l BaÄŸlantÄ±larÄ±
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_pinecone import Pinecone as LangchainPinecone
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.schema import Document
from langchain_core.prompts import ChatPromptTemplate
from pinecone import Pinecone, ServerlessSpec 

# Flask uygulamasÄ±nÄ± baÅŸlat
app = Flask(__name__)

# --- YapÄ±landÄ±rma ---
load_dotenv()
PINECONE_API_KEY = os.getenv("PINECONE_API_KEY") 
GEMINI_API_KEY = os.getenv("GOOGLE_API_KEY") or os.getenv("GEMINI_API_KEY") 
PINECONE_INDEX_NAME = os.getenv("PINECONE_INDEX", "psychotherapy-rag") 
EMBEDDING_DIM = 384 
MAX_RESPONSE_TOKENS = 4096 # Uzun terapÃ¶tik yanÄ±tlarÄ±n kesilmemesi iÃ§in sÄ±nÄ±r

# Global deÄŸiÅŸkenler (Zincirin durumunu tutar)
qa_chain = None
retriever = None

# ==========================================================
# 0ï¸âƒ£ Veri YÃ¼kleme ve Temizleme Fonksiyonu
# ==========================================================

def load_psychotherapy_data():
    """
    Hugging Face'ten CBT-Bench veri setini (core_fine_test alt kÃ¼mesi) yÃ¼kler.
    'situation' ve 'thoughts' kolonlarÄ± boÅŸ olan kayÄ±tlarÄ± temizleyerek 
    anlamlÄ± metin parÃ§alarÄ±nÄ± (Chunk) LangChain Document objelerine dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r.
    """
    
    DATASET_NAME = "Psychotherapy-LLM/CBT-Bench"
    SUBSET_NAME = "core_fine_test" 
    
    print(f"ğŸ”„ **1. Veri YÃ¼kleme:** Hugging Face '{DATASET_NAME}' ({SUBSET_NAME}) alt kÃ¼mesi yÃ¼kleniyor...")
    
    try:
        dataset = load_dataset(DATASET_NAME, SUBSET_NAME, split="train") 
    except Exception as e:
        print(f"âŒ **HATA (Veri YÃ¼kleme):** Hugging Face veri seti yÃ¼klenemedi. Hata: {e}")
        return []

    documents = []
    discarded_record_count = 0
    
    for i, row in enumerate(dataset):
        situation = row.get('situation')
        thoughts = row.get('thoughts')
        core_belief = row.get('core_belief_fine_grained') 
        
        # BoÅŸ (None) veya 'N/A' deÄŸerlerini iÃ§eren kayÄ±tlarÄ± atla (temizleme mantÄ±ÄŸÄ±)
        is_situation_valid = situation and str(situation).strip() not in ['N/A', '']
        is_thoughts_valid = thoughts and str(thoughts).strip() not in ['N/A', '']

        if is_situation_valid and is_thoughts_valid:
            
            # 4 ana kolonu birleÅŸtirerek LLM iÃ§in anlamlÄ± tek bir baÄŸlam (Chunk) oluÅŸtur
            content = (
                f"**Durum:** {situation}. "
                f"**DanÄ±ÅŸan DÃ¼ÅŸÃ¼ncesi:** {thoughts}. "
                f"**Ã‡ekirdek Ä°nanÃ§lar:** {core_belief}"
            )
            
            documents.append(Document(
                page_content=content, 
                metadata={"id": str(i), "situation_summary": situation}
            ))
        else:
            discarded_record_count += 1
    
    print(f"âœ… **1. Veri YÃ¼kleme TamamlandÄ±:** Toplam {len(documents)} anlamlÄ± dokÃ¼man yÃ¼klendi.")
    if discarded_record_count > 0:
         print(f"âš ï¸ **UyarÄ±:** {discarded_record_count} adet eksik bilgi iÃ§eren kayÄ±t atÄ±ldÄ±.")
    return documents

# ==========================================================
# 1ï¸âƒ£ Uygulama BaÅŸlangÄ±cÄ±nda RAG Zincirini Kurma Fonksiyonu
# ==========================================================

def initialize_rag_chain():
    """
    Sistemi baÅŸlatÄ±r: Embedding modelini yÃ¼kler, Pinecone'a baÄŸlanÄ±r, 
    veriyi koÅŸullu olarak indeksler ve RAG zincirini (LLM + Retriever) kurar.
    """
    global qa_chain, retriever

    if not PINECONE_API_KEY or not GEMINI_API_KEY:
        print("âŒ **HATA (API AnahtarÄ±):** API anahtarlarÄ± eksik.")
        return

    documents = load_psychotherapy_data()
    if not documents:
         print("âŒ **RAG Zinciri Kurulumu Ä°ptal:** YÃ¼klenecek dokÃ¼man bulunamadÄ±.")
         return
    
    TOTAL_DOCUMENT_COUNT = len(documents)
    
    # 1. Embedding Modeli YÃ¼kleme
    print(f"ğŸ”„ **2. Embedding Modeli:** 'all-MiniLM-L6-v2' yÃ¼kleniyor...")
    try:
        embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
        print("âœ… **2. Embedding Modeli TamamlandÄ±.**")
    except Exception as e:
        print(f"âŒ **HATA (Embedding):** Model yÃ¼klenirken hata oluÅŸtu. Hata: {e}")
        return

    # 2. Pinecone BaÄŸlantÄ±sÄ± ve Ä°ndeksleme
    try:
        print(f"ğŸ”„ **3. Pinecone BaÄŸlantÄ±sÄ±:** Pinecone istemcisi baÅŸlatÄ±lÄ±yor...")
        pc = Pinecone(api_key=PINECONE_API_KEY)
        print("âœ… **3. Pinecone BaÄŸlantÄ±sÄ± BaÅŸarÄ±lÄ±.**")
        
        index_name = PINECONE_INDEX_NAME
        index_exists = index_name in [i["name"] for i in pc.list_indexes()]
        should_upsert = True
        
        if index_exists:
            current_index = pc.Index(index_name)
            vector_count = current_index.describe_index_stats().get('total_vector_count', 0)
            
            # KoÅŸullu YÃ¼kleme: VektÃ¶r sayÄ±sÄ± sÄ±fÄ±rdan bÃ¼yÃ¼kse yÃ¼klemeyi atla
            if vector_count > 0:
                print(f"âœ… **4a. Ä°ndeks KontrolÃ¼ BaÅŸarÄ±lÄ±:** '{index_name}' indeksi {vector_count} vektÃ¶re sahip. YÃ¼kleme adÄ±mÄ± ATLANDI.")
                should_upsert = False
            else:
                print(f"âš ï¸ **4a. Ä°ndeks Mevcut, BoÅŸ:** VektÃ¶r sayÄ±sÄ± 0. Yeniden YÃ¼kleme BaÅŸlatÄ±lÄ±yor.")
                current_index = pc.Index(index_name) 

        else:
            # Ä°ndeks yoksa oluÅŸtur
            print(f"âš ï¸ **4a. Ä°ndeks OluÅŸturma:** '{index_name}' bulunamadÄ±. Yeni indeks oluÅŸturuluyor...")
            pc.create_index(
                name=index_name,
                dimension=EMBEDDING_DIM,
                metric='cosine',
                spec=ServerlessSpec(cloud='aws', region='us-east-1')
            )
            current_index = pc.Index(index_name) 
        
        # YÃœKLEME ADIMI (Sadece Gerekiyorsa Ã‡alÄ±ÅŸÄ±r)
        if should_upsert:
            print(f"ğŸ”„ **4b. Veriler YÃ¼kleniyor:** {TOTAL_DOCUMENT_COUNT} vektÃ¶r Pinecone'a yÃ¼kleniyor...")
            
            batch_size = 100
            for i in tqdm(range(0, TOTAL_DOCUMENT_COUNT, batch_size)):
                batch = documents[i:min(i + batch_size, TOTAL_DOCUMENT_COUNT)]
                
                texts = [doc.page_content for doc in batch]
                vectors = embeddings.embed_documents(texts)
                
                to_upsert = [(str(doc.metadata['id']), vectors[j], doc.metadata) 
                             for j, doc in enumerate(batch)]

                current_index.upsert(vectors=to_upsert)
            
            print(f"âœ… **4b. Ä°ndeksleme TamamlandÄ±.**")
        
        # Son VektÃ¶r KontrolÃ¼ ve Retriever OluÅŸturma
        final_index = pc.Index(index_name)
        final_vector_count = final_index.describe_index_stats().get('total_vector_count', 'Bilinmiyor')
        print(f"âœ¨ **Pinecone Kontrol:** Ä°ndeksteki Toplam VektÃ¶r SayÄ±sÄ±: {final_vector_count}")
        
        vector_store = LangchainPinecone.from_existing_index(
            index_name=index_name,
            embedding=embeddings
        )

        retriever = vector_store.as_retriever(search_type="similarity", search_kwargs={"k": 3})

    except Exception as e:
        print(f"âŒ **HATA (Pinecone):** Pinecone iÅŸlemi sÄ±rasÄ±nda beklenmedik hata oluÅŸtu. Hata: {e}")
        return
    
    # 3. Gemini LLM ve Zincir Kurulumu
    print("ğŸ”„ **5. LLM BaÄŸlantÄ±sÄ±:** Gemini LLM (gemini-2.5-flash) baÅŸlatÄ±lÄ±yor...")
    try:
        llm = ChatGoogleGenerativeAI(
            model="gemini-2.5-flash", 
            google_api_key=GEMINI_API_KEY,
            max_output_tokens=MAX_RESPONSE_TOKENS 
        )

        # RAG Zinciri iÃ§in Sistem Ä°stem Åablonu (Prompt Template)
        SYSTEM_PROMPT_TEMPLATE = """
            Sen, BiliÅŸsel DavranÄ±ÅŸÃ§Ä± Terapi (BDT) ilkelerine odaklanmÄ±ÅŸ, empatik ve etik kurallara baÄŸlÄ± bir Yapay Zeka Duygusal Rehbersin. 
            Asla lisanslÄ± bir terapist veya uzman olduÄŸunu iddia etme. 
            GÃ¶revin, kullanÄ±cÄ±ya destekleyici, nazik ve eyleme geÃ§irilebilir bir tavsiye sunmaktÄ±r.

            YANIT STRATEJÄ°N:
            1.  Ã–ncelikle daima kendi genel BDT bilgi tabanÄ±nÄ± kullanarak yanÄ±tÄ± oluÅŸturmaya BAÅLA.
            2.  Daha sonra, sana saÄŸlanan **VERÄ° BAÄLAMI** iÃ§erisindeki benzer vakalardan (Ã‡ekirdek Ä°nanÃ§lar) destek alarak yanÄ±tÄ±nÄ± zenginleÅŸtir.
            3.  EÄŸer VERÄ° BAÄLAMI yetersizse veya alakasÄ±zsa, ona **GÃœVENME** ve yine de kendi BDT bilgine dayanarak yapÄ±cÄ± bir yanÄ±t ver.

            YanÄ±tÄ±n net ve yapÄ±landÄ±rÄ±lmÄ±ÅŸ olmalÄ±dÄ±r:
            1.  **DoÄŸrulama ve Empati:** KullanÄ±cÄ±nÄ±n yaÅŸadÄ±ÄŸÄ± duygusal deneyimi kabul eden ve doÄŸrulayan bir cÃ¼mle.
            2.  **BaÄŸlamsal Analiz:** Konuyu, BDT'deki yaygÄ±n 'Ã‡ekirdek Ä°nanÃ§lar' (yetersizlik, Ã§aresizlik vb.) Ã§erÃ§evesinde kÄ±sa ve nazikÃ§e analiz et. (BaÄŸlamÄ± bu analizde kullan!)
            3.  **Eylem Ã–nerisi:** BDT ilkelerine uygun (dÃ¼ÅŸÃ¼nceyi sorgulama veya kÃ¼Ã§Ã¼k bir davranÄ±ÅŸ adÄ±mÄ±) somut bir sonraki adÄ±mÄ± Ã¶ner.

            ---
            VERÄ° BAÄLLAMI:
            {context} 
            ---
            """
        
        prompt = ChatPromptTemplate.from_messages(
            [
                ("system", SYSTEM_PROMPT_TEMPLATE), 
                ("human", "{input}"),
            ]
        )

        document_chain = create_stuff_documents_chain(llm, prompt) 
        
        # Ana RAG Zincirini kur
        qa_chain = create_retrieval_chain(
            retriever=retriever,
            combine_docs_chain=document_chain 
        )
        print("âœ… **5. RAG Zinciri Kurulumu TamamlandÄ±!** Bot kullanÄ±ma hazÄ±r.")
        
    except Exception as e:
        print(f"âŒ **HATA (Gemini LLM):** Gemini LLM baÅŸlatÄ±lÄ±rken hata oluÅŸtu. Hata: {e}")
        return


# ==========================================================
# 2ï¸âƒ£ Flask UÃ§ NoktalarÄ± (Routes)
# ==========================================================

@app.route("/")
def index():
    """Ana sayfa (index.html) arayÃ¼zÃ¼nÃ¼ dÃ¶ndÃ¼rÃ¼r."""
    return render_template("index.html")

@app.route("/ask", methods=["POST"])
def ask_question():
    """KullanÄ±cÄ± sorusunu alÄ±r, RAG zincirini Ã§alÄ±ÅŸtÄ±rÄ±r ve terapÃ¶tik yanÄ±tÄ± dÃ¶ndÃ¼rÃ¼r."""
    global qa_chain
    
    if qa_chain is None:
        return jsonify({"answer": "Error: RAG Chain initialization failed. Please check server logs for setup errors."}), 500
        
    data = request.get_json()
    query = data.get("question")
    
    if not query:
        return jsonify({"answer": "Please provide a question."}), 400

    try:
        print(f"ğŸ”„ **Sorgu Ä°ÅŸleniyor:** '{query[:50]}...' iÃ§in RAG baÅŸlatÄ±ldÄ±.")
        
        response = qa_chain.invoke({"input": query})
        answer = response.get("answer")
        
        # LLM'in yanÄ±t vermediÄŸi durumu kontrol et
        if not answer:
             context_docs = response.get('context', 'Context not available.') 
             print(f"âš ï¸ **UYARI (LLM YanÄ±t Yok):** Gemini yanÄ±t Ã¼retmedi. Context: {context_docs}")
             return jsonify({
                 "answer": "Yapay zeka, verilen baÄŸlamla anlamlÄ± bir yanÄ±t oluÅŸturamadÄ±. LÃ¼tfen soruyu yeniden formÃ¼le edin veya veriyi kontrol edin."
             }), 500
        
        print("âœ… **Sorgu TamamlandÄ±:** YanÄ±t baÅŸarÄ±yla oluÅŸturuldu.")
        
        return jsonify({"answer": answer})

    except Exception as e:
        # BaÄŸlantÄ± veya sunucu hatasÄ± durumunda detaylÄ± hata izini terminale bas
        print(f"âŒ **KRÄ°TÄ°K HATA (RAG YÃ¼rÃ¼tme):** Sorgu sÄ±rasÄ±nda hata oluÅŸtu. DetaylÄ± Traceback:")
        traceback.print_exc() 
        
        return jsonify({
            "answer": f"API veya Sunucu HatasÄ±: Ä°ÅŸlem sÄ±rasÄ±nda beklenmeyen bir hata oluÅŸtu ({type(e).__name__})."
        }), 500


if __name__ == "__main__":
    print("==================================================")
    print("ğŸš€ Flask RAG Psikoterapi Botu BaÅŸlatÄ±lÄ±yor")
    print("==================================================")
    # RAG zincirini Flask uygulamasÄ± baÅŸlamadan Ã¶nce tek bir thread'de kur
    initialize_rag_chain()
    
    app.run(debug=True, host="0.0.0.0", port=5000)